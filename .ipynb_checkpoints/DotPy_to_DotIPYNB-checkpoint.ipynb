{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ffd85528fee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# dependencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from cp_dataset import CPDataset, CPDataLoader\n",
    "# from 'cp-vton-plus' import cp_dataset\n",
    "from networks import GMM, UnetGenerator, load_checkpoint\n",
    "\n",
    "# shutil import\n",
    "import shutil\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from visualization import board_add_image, board_add_images, save_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_get\n",
    "\n",
    "def get_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--name\", default=\"GMM\")\n",
    "    # parser.add_argument(\"--name\", default=\"TOM\")\n",
    "\n",
    "    parser.add_argument(\"--gpu_ids\", default=\"\")\n",
    "    parser.add_argument('-j', '--workers', type=int, default=1)\n",
    "    parser.add_argument('-b', '--batch-size', type=int, default=4)\n",
    "\n",
    "    parser.add_argument(\"--dataroot\", default=\"data\")\n",
    "\n",
    "    # parser.add_argument(\"--datamode\", default=\"train\")\n",
    "    parser.add_argument(\"--datamode\", default=\"test\")\n",
    "\n",
    "    parser.add_argument(\"--stage\", default=\"GMM\")\n",
    "    # parser.add_argument(\"--stage\", default=\"TOM\")\n",
    "\n",
    "    # parser.add_argument(\"--data_list\", default=\"train_pairs.txt\")\n",
    "    parser.add_argument(\"--data_list\", default=\"test_pairs.txt\")\n",
    "\n",
    "    parser.add_argument(\"--fine_width\", type=int, default=192)\n",
    "    parser.add_argument(\"--fine_height\", type=int, default=256)\n",
    "    parser.add_argument(\"--radius\", type=int, default=5)\n",
    "    parser.add_argument(\"--grid_size\", type=int, default=5)\n",
    "\n",
    "    parser.add_argument('--tensorboard_dir', type=str,\n",
    "                        default='tensorboard', help='save tensorboard infos')\n",
    "\n",
    "    parser.add_argument('--result_dir', type=str,\n",
    "                        default='result', help='save result infos')\n",
    "\n",
    "    parser.add_argument('--checkpoint', type=str, default='checkpoints/GMM/gmm_final.pth', help='model checkpoint for test')\n",
    "    # parser.add_argument('--checkpoint', type=str, default='checkpoints/TOM/tom_final.pth', help='model checkpoint for test')\n",
    "\n",
    "    parser.add_argument(\"--display_count\", type=int, default=1)\n",
    "    parser.add_argument(\"--shuffle\", action='store_true',\n",
    "                        help='shuffle input data')\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self, name, gpu_ids, workers, batch_size, fine_width, fine_height, radius, grid_size, display_count, shuffle, data_root, datamode, stage, data_list, tensorboard_dir, result_dir, checkpoint):\n",
    "        self.name = name\n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.workers = workers\n",
    "        self.batch_size = batch_size\n",
    "        self.fine_width = fine_width\n",
    "        self.fine_height = fine_height\n",
    "        self.radius = radius\n",
    "        self.grid_size = grid_size\n",
    "        self.display_count = display_count\n",
    "        self.shuffle = shuffle\n",
    "        self.dataroot = data_root\n",
    "        self.datamode = datamode\n",
    "        self.stage = stage\n",
    "        self.data_list = data_list\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.checkpoint = checkpoint  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_opt(name = 'GMM', gpu_ids = '', workers =1, batch_size=4, fine_width = 192, fine_height = 256, radius = 5, grid_size = 5,  display_count = 1, shuffle= True, data_root = 'data', datamode='test', stage = 'GMM', data_list = 'test_pairs.txt', tensorboard_dir = \"tensorboard\", result_dir = \"result\", checkpoint = \"checkpoints/GMM/gmm_final.pth\"):\n",
    "    opt = Opt(name, gpu_ids, workers, batch_size, fine_width, fine_height, radius, grid_size, display_count, shuffle, data_root, datamode, stage, data_list, tensorboard_dir, result_dir, checkpoint)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_gmm funciton\n",
    "def test_gmm(opt, test_loader, model, board):\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    base_name = os.path.basename(opt.checkpoint)\n",
    "    name = opt.name\n",
    "    save_dir = os.path.join(opt.result_dir, name, opt.datamode)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    warp_cloth_dir = os.path.join(save_dir, 'warp-cloth')\n",
    "    if not os.path.exists(warp_cloth_dir):\n",
    "        os.makedirs(warp_cloth_dir)\n",
    "    warp_mask_dir = os.path.join(save_dir, 'warp-mask')\n",
    "    if not os.path.exists(warp_mask_dir):\n",
    "        os.makedirs(warp_mask_dir)\n",
    "    result_dir1 = os.path.join(save_dir, 'result_dir')\n",
    "    if not os.path.exists(result_dir1):\n",
    "        os.makedirs(result_dir1)\n",
    "    overlayed_TPS_dir = os.path.join(save_dir, 'overlayed_TPS')\n",
    "    if not os.path.exists(overlayed_TPS_dir):\n",
    "        os.makedirs(overlayed_TPS_dir)\n",
    "    warped_grid_dir = os.path.join(save_dir, 'warped_grid')\n",
    "    if not os.path.exists(warped_grid_dir):\n",
    "        os.makedirs(warped_grid_dir)\n",
    "    for step, inputs in enumerate(test_loader.data_loader):\n",
    "        iter_start_time = time.time()\n",
    "\n",
    "        c_names = inputs['c_name']\n",
    "        im_names = inputs['im_name']\n",
    "        im = inputs['image'].cuda()\n",
    "        im_pose = inputs['pose_image'].cuda()\n",
    "        im_h = inputs['head'].cuda()\n",
    "        shape = inputs['shape'].cuda()\n",
    "        agnostic = inputs['agnostic'].cuda()\n",
    "        c = inputs['cloth'].cuda()\n",
    "        cm = inputs['cloth_mask'].cuda()\n",
    "        im_c = inputs['parse_cloth'].cuda()\n",
    "        im_g = inputs['grid_image'].cuda()\n",
    "        shape_ori = inputs['shape_ori']  # original body shape without blurring\n",
    "\n",
    "        grid, theta = model(agnostic, cm)\n",
    "        warped_cloth = F.grid_sample(c, grid, padding_mode='border')\n",
    "        warped_mask = F.grid_sample(cm, grid, padding_mode='zeros')\n",
    "        warped_grid = F.grid_sample(im_g, grid, padding_mode='zeros')\n",
    "        overlay = 0.7 * warped_cloth + 0.3 * im\n",
    "\n",
    "        visuals = [[im_h, shape, im_pose],\n",
    "                   [c, warped_cloth, im_c],\n",
    "                   [warped_grid, (warped_cloth+im)*0.5, im]]\n",
    "\n",
    "        # save_images(warped_cloth, c_names, warp_cloth_dir)\n",
    "        # save_images(warped_mask*2-1, c_names, warp_mask_dir)\n",
    "        save_images(warped_cloth, im_names, warp_cloth_dir)\n",
    "        save_images(warped_mask * 2 - 1, im_names, warp_mask_dir)\n",
    "        save_images(shape_ori.cuda() * 0.2 + warped_cloth *\n",
    "                    0.8, im_names, result_dir1)\n",
    "        save_images(warped_grid, im_names, warped_grid_dir)\n",
    "        save_images(overlay, im_names, overlayed_TPS_dir)\n",
    "\n",
    "        if (step+1) % opt.display_count == 0:\n",
    "            board_add_images(board, 'combine', visuals, step+1)\n",
    "            t = time.time() - iter_start_time\n",
    "            print('step: %8d, time: %.3f' % (step+1, t), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tom\n",
    "\n",
    "def test_tom(opt, test_loader, model, board):\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    base_name = os.path.basename(opt.checkpoint)\n",
    "    # save_dir = os.path.join(opt.result_dir, base_name, opt.datamode)\n",
    "    save_dir = os.path.join(opt.result_dir, opt.name, opt.datamode)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    try_on_dir = os.path.join(save_dir, 'try-on')\n",
    "    if not os.path.exists(try_on_dir):\n",
    "        os.makedirs(try_on_dir)\n",
    "    p_rendered_dir = os.path.join(save_dir, 'p_rendered')\n",
    "    if not os.path.exists(p_rendered_dir):\n",
    "        os.makedirs(p_rendered_dir)\n",
    "    m_composite_dir = os.path.join(save_dir, 'm_composite')\n",
    "    if not os.path.exists(m_composite_dir):\n",
    "        os.makedirs(m_composite_dir)\n",
    "    im_pose_dir = os.path.join(save_dir, 'im_pose')\n",
    "    if not os.path.exists(im_pose_dir):\n",
    "        os.makedirs(im_pose_dir)\n",
    "    shape_dir = os.path.join(save_dir, 'shape')\n",
    "    if not os.path.exists(shape_dir):\n",
    "        os.makedirs(shape_dir)\n",
    "    im_h_dir = os.path.join(save_dir, 'im_h')\n",
    "    if not os.path.exists(im_h_dir):\n",
    "        os.makedirs(im_h_dir)  # for test data\n",
    "\n",
    "    print('Dataset size: %05d!' % (len(test_loader.dataset)), flush=True)\n",
    "    for step, inputs in enumerate(test_loader.data_loader):\n",
    "        iter_start_time = time.time()\n",
    "\n",
    "        im_names = inputs['im_name']\n",
    "        im = inputs['image'].cuda()\n",
    "        im_pose = inputs['pose_image']\n",
    "        im_h = inputs['head']\n",
    "        shape = inputs['shape']\n",
    "\n",
    "        agnostic = inputs['agnostic'].cuda()\n",
    "        c = inputs['cloth'].cuda()\n",
    "        cm = inputs['cloth_mask'].cuda()\n",
    "\n",
    "        # outputs = model(torch.cat([agnostic, c], 1))  # CP-VTON\n",
    "        outputs = model(torch.cat([agnostic, c, cm], 1))  # CP-VTON+\n",
    "        p_rendered, m_composite = torch.split(outputs, 3, 1)\n",
    "        p_rendered = F.tanh(p_rendered)\n",
    "        m_composite = F.sigmoid(m_composite)\n",
    "        p_tryon = c * m_composite + p_rendered * (1 - m_composite)\n",
    "\n",
    "        visuals = [[im_h, shape, im_pose],\n",
    "                   [c, 2*cm-1, m_composite],\n",
    "                   [p_rendered, p_tryon, im]]\n",
    "\n",
    "        save_images(p_tryon, im_names, try_on_dir)\n",
    "        save_images(im_h, im_names, im_h_dir)\n",
    "        save_images(shape, im_names, shape_dir)\n",
    "        save_images(im_pose, im_names, im_pose_dir)\n",
    "        save_images(m_composite, im_names, m_composite_dir)\n",
    "        save_images(p_rendered, im_names, p_rendered_dir)  # For test data\n",
    "\n",
    "        if (step+1) % opt.display_count == 0:\n",
    "            board_add_images(board, 'combine', visuals, step+1)\n",
    "            t = time.time() - iter_start_time\n",
    "            print('step: %8d, time: %.3f' % (step+1, t), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-13-2fbf41d84eb1>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-2fbf41d84eb1>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    test_gmm(opt, test_loader, model, board)\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def run_model(opt):\n",
    "    print(\"Start to test stage: %s, named: %s!\" % (opt.stage, opt.name))\n",
    "    test_dataset = CPDataset(opt)\n",
    "\n",
    "    # create dataloader\n",
    "    test_loader = CPDataLoader(opt, test_dataset)\n",
    "\n",
    "    # visualization\n",
    "    if not os.path.exists(opt.tensorboard_dir):\n",
    "        os.makedirs(opt.tensorboard_dir)\n",
    "    board = SummaryWriter(logdir=os.path.join(opt.tensorboard_dir, opt.name))\n",
    "\n",
    "    # create model & test\n",
    "    if opt.stage == 'GMM':\n",
    "        model = GMM(opt)\n",
    "        load_checkpoint(model, opt.checkpoint)\n",
    "        with torch.no_grad():\n",
    "        test_gmm(opt, test_loader, model, board)\n",
    "    elif opt.stage == 'TOM':\n",
    "        # model = UnetGenerator(25, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)  # CP-VTON\n",
    "        model = UnetGenerator(26, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)  # CP-VTON+\n",
    "        load_checkpoint(model, opt.checkpoint)\n",
    "        with torch.no_grad():\n",
    "        test_tom(opt, test_loader, model, board)\n",
    "    else:\n",
    "        raise NotImplementedError('Model [%s] is not implemented' % opt.stage)\n",
    "\n",
    "    print('Finished test %s, named: %s!' % (opt.stage, opt.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "def main():\n",
    "    for i in range(2):      \n",
    "        # opt = add_opt(name='GMM', stage='GMM', checkpoint='checkpoints/GMM/gmm_final.pth') if i == 0 else add_opt(name='TOM', stage='TOM', checkpoint='checkpoints/TOM/tom_final.pth')\n",
    "        # run_model(opt)\n",
    "        if i == 0:\n",
    "            opt = add_opt(name='GMM', stage='GMM', checkpoint='checkpoints/GMM/gmm_final.pth')\n",
    "            run_model(opt)\n",
    "            # move model\n",
    "            source = './result/GMM/test/warp-cloth'\n",
    "            dest = './data'\n",
    "            shutil.move(source, dest)\n",
    "        else :\n",
    "            add_opt(name='TOM', stage='TOM', checkpoint='checkpoints/TOM/tom_final.pth')\n",
    "            run_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to test stage: GMM, named: GMM!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CPDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4948a67c86b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Starter Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-d640b99b82ff>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'GMM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'GMM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'checkpoints/GMM/gmm_final.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;31m# move model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./result/GMM/test/warp-cloth'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-3aa8dbb52145>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start to test stage: %s, named: %s!\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCPDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;31m# create dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CPDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Starter Function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensoflow1",
   "language": "python",
   "name": "tensorflow1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
